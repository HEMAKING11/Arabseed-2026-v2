# arabseed_telegram_bot_final.py
import os
import re
import sys
import json
import time
import random
import logging
import traceback
from urllib.parse import urlparse, unquote, urlunparse, quote, parse_qs
from typing import Dict, List, Optional, Tuple, Any
from datetime import datetime

import requests
from bs4 import BeautifulSoup
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import (
    Application,
    CommandHandler,
    MessageHandler,
    CallbackQueryHandler,
    filters,
    ContextTypes,
)

# ----------------- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ³Ø¬ÙŠÙ„ -----------------
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# ----------------- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¨ÙˆØª -----------------
TOKEN = os.environ.get("TELEGRAM_BOT_TOKEN", "7064549403:AAHWQsrZPekW1M9kHacqB6N19aMj_xjspf4")

# ----------------- Ù‚Ø§Ø¦Ù…Ø© User-Agents -----------------
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 17_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Mobile/15E148 Safari/604.1",
]

# ----------------- Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø¬Ù„Ø³Ø§Øª -----------------
class UserSession:
    def __init__(self):
        self.processing = False
        self.auto_mode = False
        self.current_episode = 0
        self.builder_func = None
        self.last_url = ""
        self.last_title = ""
        self.history = []
        
    def reset(self):
        self.processing = False
        self.auto_mode = False
        self.current_episode = 0
        self.builder_func = None

user_sessions = {}

def get_user_session(user_id: int) -> UserSession:
    if user_id not in user_sessions:
        user_sessions[user_id] = UserSession()
    return user_sessions[user_id]

# ----------------- Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø© -----------------
def extract_base_url(url: str) -> str:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ"""
    parsed_url = urlparse(url)
    return f"{parsed_url.scheme}://{parsed_url.netloc}"

def extract_title_from_url(url: str) -> str:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø·"""
    try:
        parsed_url = urlparse(url)
        path = unquote(parsed_url.path)
        path_parts = path.strip('/').split('-')
        title = ' '.join(path_parts).replace('.html', '').replace('.php', '').title()
        
        if title.startswith("Ù…Ø³Ù„Ø³Ù„"):
            words = title.split()
            new_title = []
            for word in words:
                new_title.append(word)
                if any(char.isdigit() for char in word):
                    break
            title = ' '.join(new_title)
        return title
    except:
        return "Ø¹Ù†ÙˆØ§Ù† ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ"

def get_random_headers() -> Dict:
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‡ÙŠØ¯Ø±Ø§Øª Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©"""
    return {
        "User-Agent": random.choice(USER_AGENTS),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "ar,en-US;q=0.7,en;q=0.3",
        "Accept-Encoding": "gzip, deflate, br",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
        "Cache-Control": "max-age=0",
        "TE": "Trailers",
    }

def make_request(url: str, max_retries: int = 3, session: Optional[requests.Session] = None) -> Optional[requests.Response]:
    """Ø·Ù„Ø¨ Ù…Ø­Ø³Ù† Ù…Ø¹ Ø¥Ø¹Ø§Ø¯Ø© Ù…Ø­Ø§ÙˆÙ„Ø©"""
    headers = get_random_headers()
    
    for attempt in range(max_retries):
        try:
            if session:
                response = session.get(
                    url,
                    headers=headers,
                    timeout=20,
                    allow_redirects=True,
                    verify=False  # Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† SSL Ù…Ø¤Ù‚ØªØ§Ù‹
                )
            else:
                response = requests.get(
                    url,
                    headers=headers,
                    timeout=20,
                    allow_redirects=True,
                    verify=False
                )
            
            if response.status_code == 200:
                return response
            elif response.status_code == 403:
                logger.warning(f"403 Forbidden on attempt {attempt + 1}")
                time.sleep(2 ** attempt)  # Ø²ÙŠØ§Ø¯Ø© ÙˆÙ‚Øª Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± ØªØ¯Ø±ÙŠØ¬ÙŠØ§Ù‹
                headers = get_random_headers()  # ØªØºÙŠÙŠØ± Ø§Ù„Ù‡ÙŠØ¯Ø±Ø§Øª
            else:
                logger.warning(f"Status {response.status_code} on attempt {attempt + 1}")
                time.sleep(1)
                
        except requests.exceptions.RequestException as e:
            logger.error(f"Request error on attempt {attempt + 1}: {e}")
            time.sleep(2 ** attempt)
    
    return None

def find_last_numeric_segment_in_path(path_unquoted: str) -> Tuple[Optional[int], Optional[str]]:
    """Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø±Ù‚Ù…ÙŠ Ø§Ù„Ø£Ø®ÙŠØ± ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø±"""
    parts = path_unquoted.strip('/').split('-')
    for i in range(len(parts)-1, -1, -1):
        if re.fullmatch(r'\d+', parts[i]):
            return i, parts[i]
    return None, None

def build_episode_url_from_any(url: str, episode_number: int) -> Optional[str]:
    """Ø¨Ù†Ø§Ø¡ Ø±Ø§Ø¨Ø· Ø§Ù„Ø­Ù„Ù‚Ø©"""
    p = urlparse(url)
    path_unquoted = unquote(p.path)
    idx, num = find_last_numeric_segment_in_path(path_unquoted)
    if idx is None:
        return None
    parts = path_unquoted.strip('/').split('-')[:idx+1]
    parts[-1] = str(episode_number)
    new_path = '/' + '-'.join(parts)
    quoted_path = quote(new_path, safe="/%")
    new_parsed = (p.scheme, p.netloc, quoted_path, '', '', '')
    return urlunparse(new_parsed)

def extract_episode_and_base(url: str) -> Tuple[Optional[int], Optional[callable]]:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±Ù‚Ù… Ø§Ù„Ø­Ù„Ù‚Ø© ÙˆØ¯Ø§Ù„Ø© Ø§Ù„Ø¨Ù†Ø§Ø¡"""
    p = urlparse(url)
    path_unquoted = unquote(p.path)
    idx, num = find_last_numeric_segment_in_path(path_unquoted)
    if idx is None or num is None:
        return None, None
    return int(num), lambda ep: build_episode_url_from_any(url, ep)

# ----------------- Ø¯Ø§Ù„Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø­Ø³Ù†Ø© -----------------
def get_download_info(server_href: str, referer: str) -> Optional[Dict]:
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ù† Ø±Ø§Ø¨Ø· Ø§Ù„Ø³ÙŠØ±ÙØ±"""
    try:
        session = requests.Session()
        headers = get_random_headers()
        headers["Referer"] = referer
        session.headers.update(headers)
        
        logger.info(f"ğŸ” Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø©: {server_href}")
        
        # Ø§Ù„Ø®Ø·ÙˆØ© 1: ØªØªØ¨Ø¹ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªÙˆØ¬ÙŠÙ‡
        try:
            response = session.get(server_href, timeout=15, allow_redirects=False)
            if response.status_code in [301, 302, 303, 307, 308] and 'location' in response.headers:
                redirected_url = response.headers['location']
                if not redirected_url.startswith('http'):
                    base = extract_base_url(server_href)
                    redirected_url = base + redirected_url
                logger.info(f"â†ªï¸ ØªÙ… Ø§Ù„ØªÙˆØ¬ÙŠÙ‡ Ø¥Ù„Ù‰: {redirected_url}")
                server_href = redirected_url
        except:
            pass
        
        # Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
        response = make_request(server_href, session=session)
        if not response:
            return None
        
        html_content = response.text
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø±Ø§Ø¨Ø· ?r= Ø£Ùˆ downloadz
        r_link = None
        patterns = [
            r'(https?://[^"\'>\s]+/category/downloadz/\?r=\d+[^"\'>\s]*)',
            r'(https?://[^"\'>\s]+\?r=\d+[^"\'>\s]*)',
            r'href=["\']([^"\']+downloadz[^"\']*)["\']',
            r'window\.location\s*=\s*["\']([^"\']+)["\']',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            if matches:
                r_link = matches[0]
                if r_link.startswith('//'):
                    r_link = 'https:' + r_link
                elif not r_link.startswith('http'):
                    r_link = extract_base_url(server_href) + r_link
                break
        
        if not r_link:
            # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ø­Ø§Ù„ÙŠ
            r_link = response.url
        
        logger.info(f"âœ… ÙˆØ¬Ø¯Øª Ø±Ø§Ø¨Ø· Ø§Ù„ØªØ­Ù…ÙŠÙ„: {r_link}")
        
        # Ø§Ù„Ø®Ø·ÙˆØ© 3: Ø¬Ù„Ø¨ ØµÙØ­Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„
        time.sleep(0.5)  # ØªØ£Ø®ÙŠØ± Ø¨Ø³ÙŠØ·
        response = make_request(r_link, session=session)
        if not response:
            return None
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø±Ø§Ø¨Ø· Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
        final_link = None
        
        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
        for a in soup.find_all('a', href=True):
            href = a['href']
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø±ÙˆØ§Ø¨Ø· MP4 Ø£Ùˆ direct
            if re.search(r'\.(mp4|m3u8|mkv|avi)$', href, re.IGNORECASE) or 'direct' in href.lower() or 'download' in href.lower():
                final_link = href
                if not final_link.startswith('http'):
                    final_link = extract_base_url(r_link) + final_link
                break
        
        # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ØŒ Ù†Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ©
        if not final_link:
            script_tags = soup.find_all('script')
            for script in script_tags:
                if script.string:
                    # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ JavaScript
                    patterns = [
                        r'src=["\']([^"\']+\.mp4[^"\']*)["\']',
                        r'file["\']?\s*:\s*["\']([^"\']+\.mp4[^"\']*)["\']',
                        r'url["\']?\s*:\s*["\']([^"\']+\.mp4[^"\']*)["\']',
                        r'["\']?(?:file|url|src)["\']?\s*:\s*["\']([^"\']+)["\']',
                    ]
                    for pattern in patterns:
                        match = re.search(pattern, script.string, re.IGNORECASE)
                        if match:
                            final_link = match.group(1)
                            if not final_link.startswith('http'):
                                final_link = extract_base_url(r_link) + final_link
                            break
                if final_link:
                    break
        
        # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ Ø¨Ø¹Ø¯ØŒ Ù†Ø³ØªØ®Ø¯Ù… Ø¨Ø¹Ø¶ Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø§Ù„Ø¨Ø¯ÙŠÙ„Ø©
        if not final_link:
            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù† iframe
            iframe = soup.find('iframe', src=True)
            if iframe:
                final_link = iframe['src']
                if not final_link.startswith('http'):
                    final_link = extract_base_url(r_link) + final_link
        
        if not final_link:
            logger.error("âŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±Ø§Ø¨Ø· Ø§Ù„ØªØ­Ù…ÙŠÙ„")
            return None
        
        logger.info(f"ğŸ¯ Ø±Ø§Ø¨Ø· Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: {final_link}")
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù„Ù
        file_name = None
        file_size = None
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¹Ù†ÙˆØ§Ù† ÙˆØ§Ù„Ø­Ø¬Ù…
        title_elem = soup.find(['h1', 'h2', 'h3', 'title'])
        if title_elem:
            title_text = title_elem.get_text(strip=True)
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù Ù…Ù† Ø§Ù„Ù†Øµ
            size_match = re.search(r'(\d+(?:\.\d+)?)\s*(MB|GB|KB)', title_text, re.IGNORECASE)
            if size_match:
                file_size = f"{size_match.group(1)} {size_match.group(2).upper()}"
            
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¹Ù†ÙˆØ§Ù† ÙƒØ§Ø³Ù… Ù…Ù„Ù
            file_name = title_text[:50]  # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø·ÙˆÙ„
        
        # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ Ø§Ø³Ù…Ø§Ù‹ØŒ Ù†Ø³ØªØ®Ø¯Ù… Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø·
        if not file_name:
            file_name = os.path.basename(final_link).split('?')[0] or "Ù…Ù„Ù_ØªØ­Ù…ÙŠÙ„"
        
        # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ Ø­Ø¬Ù…Ø§Ù‹ØŒ Ù†Ø³ØªØ®Ø¯Ù… Ù‚ÙŠÙ…Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ©
        if not file_size:
            file_size = "ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ"
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø±Ø§Ø¨Ø·
        final_link = final_link.replace(" ", "%20")
        
        return {
            'direct_link': final_link,
            'file_name': file_name,
            'file_size': file_size
        }
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ­Ù…ÙŠÙ„: {e}")
        return None

# ----------------- Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© -----------------
def process_arabseed_url(url: str) -> Tuple[bool, str, List[List[Dict]]]:
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø±Ø§Ø¨Ø· Ø¹Ø±Ø¨ Ø³ÙŠØ¯"""
    session = requests.Session()
    headers = get_random_headers()
    session.headers.update(headers)
    
    try:
        logger.info(f"ğŸš€ Ø¨Ø¯Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø§Ø¨Ø·: {url}")
        
        # Ø§Ù„Ø®Ø·ÙˆØ© 1: Ø¬Ù„Ø¨ ØµÙØ­Ø© Ø§Ù„Ø­Ù„Ù‚Ø©
        response = make_request(url, session=session)
        if not response:
            return False, "âŒ ØªØ¹Ø°Ø± Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ø§Ù„Ø±Ø§Ø¨Ø·ØŒ ØªØ£ÙƒØ¯ Ù…Ù† ØµØ­ØªÙ‡", []
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„ØµÙØ­Ø© Ù…ÙˆØ¬ÙˆØ¯Ø©
        if response.status_code != 200:
            return False, f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„ØµÙØ­Ø© (Ø±Ù…Ø²: {response.status_code})", []
        
        html_content = response.text
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø­Ù„Ù‚Ø©
        error_indicators = [
            'Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ±',
            'ØµÙØ­Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©',
            'not found',
            '404',
            'error',
            'Ø¹Ø°Ø±Ø§Ù‹'
        ]
        
        page_text = soup.get_text().lower()
        if any(indicator in page_text for indicator in error_indicators):
            return False, "âŒ Ø§Ù„Ø­Ù„Ù‚Ø© ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© Ø£Ùˆ Ø§Ù„Ø±Ø§Ø¨Ø· ØºÙŠØ± ØµØ­ÙŠØ­", []
        
        # Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØªØ­Ù…ÙŠÙ„
        download_links = []
        
        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
        for a in soup.find_all('a', href=True):
            href = a['href'].lower()
            if any(keyword in href for keyword in ['download', 'ØªØ­Ù…ÙŠÙ„', 'server', 'Ø³ÙŠØ±ÙØ±', 'Ø¬ÙˆØ¯Ø©', 'quality']):
                download_links.append(a['href'])
        
        # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ØŒ Ù†Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø£Ø²Ø±Ø§Ø±
        if not download_links:
            buttons = soup.find_all(['button', 'a'], text=re.compile(r'ØªØ­Ù…ÙŠÙ„|ØªÙ†Ø²ÙŠÙ„|download', re.IGNORECASE))
            for btn in buttons:
                if btn.get('onclick'):
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±Ø§Ø¨Ø· Ù…Ù† onclick
                    match = re.search(r"location\.href=['\"]([^'\"]+)['\"]", btn.get('onclick', ''))
                    if match:
                        download_links.append(match.group(1))
        
        # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ Ø¨Ø¹Ø¯ØŒ Ù†Ø³ØªØ®Ø¯Ù… Ø¨Ø¹Ø¶ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
        if not download_links:
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£ÙŠ Ø±Ø§Ø¨Ø· ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ /download/
            for a in soup.find_all('a', href=re.compile(r'/download/', re.IGNORECASE)):
                download_links.append(a['href'])
        
        if not download_links:
            return False, "âŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØªØ­Ù…ÙŠÙ„ ÙÙŠ Ø§Ù„ØµÙØ­Ø©", []
        
        logger.info(f"ğŸ”— ÙˆØ¬Ø¯Øª {len(download_links)} Ø±ÙˆØ§Ø¨Ø· ØªØ­Ù…ÙŠÙ„")
        
        # Ø§Ù„Ø®Ø·ÙˆØ© 3: Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ Ø±Ø§Ø¨Ø·
        buttons_data = []
        base_url = extract_base_url(url)
        
        for i, link in enumerate(download_links[:5]):  # Ù†Ø£Ø®Ø° Ø£ÙˆÙ„ 5 Ø±ÙˆØ§Ø¨Ø· ÙÙ‚Ø·
            if not link.startswith('http'):
                link = base_url + link
            
            logger.info(f"âš™ï¸ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø§Ø¨Ø· {i+1}: {link}")
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ­Ù…ÙŠÙ„
            info = get_download_info(link, base_url + "/")
            
            if info and info.get('direct_link'):
                # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¬ÙˆØ¯Ø©
                quality = "Ø¬ÙˆØ¯Ø© Ø¹Ø§Ù„ÙŠØ©"
                if '360' in link or '360' in info['file_name']:
                    quality = "360p"
                elif '480' in link or '480' in info['file_name']:
                    quality = "480p"
                elif '720' in link or '720' in info['file_name']:
                    quality = "720p"
                elif '1080' in link or '1080' in info['file_name']:
                    quality = "1080p"
                
                # Ø¥Ù†Ø´Ø§Ø¡ Ø²Ø±
                btn_text = f"ğŸ“¥ {quality} - {info['file_size']}"
                buttons_data.append([{"text": btn_text, "url": info['direct_link']}])
                
                logger.info(f"âœ… ØªÙ… Ø¥Ø¶Ø§ÙØ© {quality}")
            
            # ØªØ£Ø®ÙŠØ± Ø¨Ø³ÙŠØ· Ø¨ÙŠÙ† Ø§Ù„Ø·Ù„Ø¨Ø§Øª
            time.sleep(0.3)
        
        if not buttons_data:
            return False, "âŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±ÙˆØ§Ø¨Ø· ØªØ­Ù…ÙŠÙ„ ØµØ§Ù„Ø­Ø©", []
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
        title = extract_title_from_url(url)
        
        logger.info(f"ğŸ‰ ØªÙ…Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¨Ù†Ø¬Ø§Ø­: {title}")
        return True, title, buttons_data
        
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø§Ø¨Ø·: {e}")
        return False, f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {str(e)}", []

# ----------------- Ø¯ÙˆØ§Ù„ Telegram -----------------
async def start_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Ø£Ù…Ø± /start"""
    user = update.effective_user
    welcome_text = f"""
ğŸ¬ *Ù…Ø±Ø­Ø¨Ø§Ù‹ {user.first_name}!* 

ğŸ¤– *Ø¨ÙˆØª ØªØ­Ù…ÙŠÙ„ Ø¹Ø±Ø¨ Ø³ÙŠØ¯ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±*

ğŸ”— *ÙƒÙŠÙÙŠØ© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:*
1. Ø£Ø±Ø³Ù„ Ø±Ø§Ø¨Ø· Ø­Ù„Ù‚Ø© Ù…Ù† Ù…ÙˆÙ‚Ø¹ Ø¹Ø±Ø¨ Ø³ÙŠØ¯
2. Ø§Ù†ØªØ¸Ø± Ù‚Ù„ÙŠÙ„Ø§Ù‹ Ø­ØªÙ‰ ØªØªÙ… Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
3. Ø§Ø®ØªØ± Ø¬ÙˆØ¯Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ù† Ø§Ù„Ø£Ø²Ø±Ø§Ø±

ğŸ“Œ *Ù…Ø«Ø§Ù„ Ù„Ù„Ø±Ø§Ø¨Ø·:*
`https://arabseed.top/Ù…Ø³Ù„Ø³Ù„-Ø§Ù„Ø¹Ù†ÙƒØ¨ÙˆØª-Ø§Ù„Ø­Ù„Ù‚Ø©-1`
Ø£Ùˆ
`https://arabseed.cam/Ù…Ø³Ù„Ø³Ù„-Ø§Ù„Ø¹Ù†ÙƒØ¨ÙˆØª-Ø§Ù„Ø­Ù„Ù‚Ø©-1.html`

ğŸ¯ *Ù…Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¨ÙˆØª:*
â€¢ ØªØ­Ù…ÙŠÙ„ Ù…Ø¨Ø§Ø´Ø± Ø¨Ø¬ÙˆØ¯Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø©
â€¢ Ø¯Ø¹Ù… Ø¬Ù…ÙŠØ¹ Ø±ÙˆØ§Ø¨Ø· Ø¹Ø±Ø¨ Ø³ÙŠØ¯
â€¢ ÙˆØ§Ø¬Ù‡Ø© Ø³Ù‡Ù„Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
â€¢ ÙŠØ¹Ù…Ù„ 24/7

âš¡ *Ù„Ù„Ø¨Ø¯Ø¡:* Ø£Ø±Ø³Ù„ Ø±Ø§Ø¨Ø· Ø§Ù„Ø­Ù„Ù‚Ø© Ø§Ù„Ø¢Ù†!
    """
    
    keyboard = [
        [InlineKeyboardButton("ğŸ¬ Ø¥Ø±Ø³Ø§Ù„ Ø±Ø§Ø¨Ø·", switch_inline_query_current_chat="")],
        [InlineKeyboardButton("ğŸ“¢ Ù‚Ù†Ø§Ø© Ø§Ù„Ø¯Ø¹Ù…", url="https://t.me/arabseed_support")]
    ]
    reply_markup = InlineKeyboardMarkup(keyboard)
    
    await update.message.reply_text(welcome_text, reply_markup=reply_markup, parse_mode='Markdown')

async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Ø£Ù…Ø± /help"""
    help_text = """
ğŸ“– *ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¨ÙˆØª:*

1. ğŸ” Ø§Ø°Ù‡Ø¨ Ø¥Ù„Ù‰ Ù…ÙˆÙ‚Ø¹ Ø¹Ø±Ø¨ Ø³ÙŠØ¯ (arabseed.top Ø£Ùˆ arabseed.cam)
2. ğŸ“‹ Ø§Ù†Ø³Ø® Ø±Ø§Ø¨Ø· Ø§Ù„Ø­Ù„Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
3. ğŸ“© Ø£Ø±Ø³Ù„ Ø§Ù„Ø±Ø§Ø¨Ø· Ù‡Ù†Ø§ ÙÙŠ Ø§Ù„Ø¨ÙˆØª
4. â³ Ø§Ù†ØªØ¸Ø± Ù‚Ù„ÙŠÙ„Ø§Ù‹ (10-20 Ø«Ø§Ù†ÙŠØ©)
5. ğŸ“¥ Ø§Ø®ØªØ± Ø¬ÙˆØ¯Ø© Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©

âš ï¸ *Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ù…Ù‡Ù…Ø©:*
â€¢ Ø§Ù„Ø¨ÙˆØª Ù„Ø§ ÙŠØ®Ø²Ù† Ø£ÙŠ Ù…Ù„ÙØ§Øª
â€¢ Ø§Ù„Ø¬ÙˆØ¯Ø© ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ù…ØµØ¯Ø± Ø§Ù„Ø£ØµÙ„ÙŠ
â€¢ Ù‚Ø¯ Ù„Ø§ ØªØ¹Ù…Ù„ Ø¨Ø¹Ø¶ Ø§Ù„Ø­Ù„Ù‚Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©

ğŸ”„ *ÙÙŠ Ø­Ø§Ù„Ø© ÙˆØ¬ÙˆØ¯ Ù…Ø´ÙƒÙ„Ø©:*
1. ØªØ£ÙƒØ¯ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø±Ø§Ø¨Ø·
2. Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ø¨Ø¹Ø¯ Ù‚Ù„ÙŠÙ„
3. ØªØ£ÙƒØ¯ Ø£Ù† Ø§Ù„Ø­Ù„Ù‚Ø© Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆÙ‚Ø¹
4. ØªÙˆØ§ØµÙ„ Ù…Ø¹ Ø§Ù„Ø¯Ø¹Ù… @arabseed_support

ğŸ¬ *Ù…ÙˆØ§Ù‚Ø¹ Ù…Ø¯Ø¹ÙˆÙ…Ø©:*
â€¢ arabseed.top
â€¢ arabseed.cam
â€¢ arabseed.ink
â€¢ ÙˆØ£ÙŠ Ù…ÙˆÙ‚Ø¹ Ø¹Ø±Ø¨ Ø³ÙŠØ¯ Ø¢Ø®Ø±
    """
    
    await update.message.reply_text(help_text, parse_mode='Markdown')

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø³Ø§Ø¦Ù„"""
    user_id = update.effective_user.id
    session = get_user_session(user_id)
    
    if session.processing:
        await update.message.reply_text("â³ Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ùƒ Ø§Ù„Ø³Ø§Ø¨Ù‚ØŒ Ø§Ù†ØªØ¸Ø± Ù‚Ù„ÙŠÙ„Ø§Ù‹...")
        return
    
    session.processing = True
    url = update.message.text.strip()
    
    try:
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ø±Ø§Ø¨Ø· ØµØ§Ù„Ø­
        if not url.startswith(('http://', 'https://')):
            await update.message.reply_text("âŒ Ù‡Ø°Ø§ Ù„ÙŠØ³ Ø±Ø§Ø¨Ø·Ø§Ù‹ ØµØ§Ù„Ø­Ø§Ù‹! ÙŠØ±Ø¬Ù‰ Ø¥Ø±Ø³Ø§Ù„ Ø±Ø§Ø¨Ø· ÙŠØ¨Ø¯Ø£ Ø¨Ù€ http:// Ø£Ùˆ https://")
            session.processing = False
            return
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ø±Ø§Ø¨Ø· ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ arabseed
        if 'arabseed' not in url.lower():
            await update.message.reply_text("âš ï¸ ÙŠØ¨Ø¯Ùˆ Ø£Ù† Ù‡Ø°Ø§ Ø§Ù„Ø±Ø§Ø¨Ø· Ù„ÙŠØ³ Ù…Ù† Ù…ÙˆÙ‚Ø¹ Ø¹Ø±Ø¨ Ø³ÙŠØ¯. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø· ÙˆØ­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.")
            session.processing = False
            return
        
        # Ø¥Ø±Ø³Ø§Ù„ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø±
        wait_msg = await update.message.reply_text("â³ Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø§Ø¨Ø·ØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± 10-20 Ø«Ø§Ù†ÙŠØ©...")
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø±Ø§Ø¨Ø·
        success, title_or_msg, buttons_data = process_arabseed_url(url)
        
        if success:
            # Ø­Ø°Ù Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø±
            await wait_msg.delete()
            
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ø£Ø²Ø±Ø§Ø± Telegram
            keyboard = []
            for button_row in buttons_data:
                row = []
                for button in button_row:
                    # ØªÙ†Ø¸ÙŠÙ Ù†Øµ Ø§Ù„Ø²Ø±
                    clean_text = button["text"].replace("[", "").replace("]", "").strip()
                    row.append(InlineKeyboardButton(clean_text, url=button["url"]))
                keyboard.append(row)
            
            # Ø¥Ø¶Ø§ÙØ© Ø£Ø²Ø±Ø§Ø± Ø¥Ø¶Ø§ÙÙŠØ©
            keyboard.append([
                InlineKeyboardButton("ğŸ”„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø±Ø§Ø¨Ø· Ø¢Ø®Ø±", callback_data="new_link"),
                InlineKeyboardButton("ğŸ“¢ Ù‚Ù†Ø§Ø© Ø§Ù„Ø¯Ø¹Ù…", url="https://t.me/arabseed_support")
            ])
            
            reply_markup = InlineKeyboardMarkup(keyboard)
            
            # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ù…Ø¹ Ø§Ù„Ø£Ø²Ø±Ø§Ø±
            message_text = f"""
ğŸ¬ *{title_or_msg}*

ğŸ“¥ *Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ØªØ§Ø­Ø©:*
Ø§Ø®ØªØ± Ø§Ù„Ø¬ÙˆØ¯Ø© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø© Ù…Ù† Ø§Ù„Ø£Ø²Ø±Ø§Ø± Ø£Ø¯Ù†Ø§Ù‡.

ğŸ”” *Ù…Ù„Ø§Ø­Ø¸Ø©:* Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù…Ø¨Ø§Ø´Ø±Ø© Ù…Ù† Ø³ÙŠØ±ÙØ±Ø§Øª Ø¹Ø±Ø¨ Ø³ÙŠØ¯
            """
            
            await update.message.reply_text(
                message_text,
                reply_markup=reply_markup,
                parse_mode='Markdown'
            )
            
            # Ø­ÙØ¸ ÙÙŠ Ø§Ù„ØªØ§Ø±ÙŠØ®
            session.history.append({
                'url': url,
                'title': title_or_msg,
                'time': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            })
            
        else:
            await wait_msg.delete()
            
            # Ø±Ø³Ø§Ù„Ø© Ø®Ø·Ø£ Ø£ÙƒØ«Ø± ÙˆØµÙÙŠØ©
            error_text = f"""
{title_or_msg}

ğŸ” *Ù†ØµØ§Ø¦Ø­ Ù„Ø­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©:*
1. ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø±Ø§Ø¨Ø· ÙŠØ¹Ù…Ù„ ÙÙŠ Ù…ØªØµÙØ­Ùƒ
2. ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ø­Ù„Ù‚Ø© Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆÙ‚Ø¹
3. Ø­Ø§ÙˆÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø±Ø§Ø¨Ø· Ù…Ø®ØªÙ„Ù Ù„Ù†ÙØ³ Ø§Ù„Ø­Ù„Ù‚Ø©
4. Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ù…Ø¹Ø·Ù„ Ù…Ø¤Ù‚ØªØ§Ù‹

ğŸ”„ Ø¬Ø±Ø¨ Ø±Ø§Ø¨Ø·Ø§Ù‹ Ø¢Ø®Ø± Ù…Ù† Ù…ÙˆÙ‚Ø¹ Ø¹Ø±Ø¨ Ø³ÙŠØ¯
            """
            
            keyboard = [
                [InlineKeyboardButton("ğŸ”„ Ù…Ø­Ø§ÙˆÙ„Ø© Ø¨Ø±Ø§Ø¨Ø· Ø¢Ø®Ø±", callback_data="new_link")],
                [InlineKeyboardButton("ğŸ“¢ Ù‚Ù†Ø§Ø© Ø§Ù„Ø¯Ø¹Ù…", url="https://t.me/arabseed_support")]
            ]
            reply_markup = InlineKeyboardMarkup(keyboard)
            
            await update.message.reply_text(error_text, reply_markup=reply_markup, parse_mode='Markdown')
            
    except Exception as e:
        logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ handle_message: {e}\n{traceback.format_exc()}")
        await update.message.reply_text("âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.")
        
    finally:
        session.processing = False

async def handle_callback(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¶ØºØ·Ø§Øª Ø§Ù„Ø£Ø²Ø±Ø§Ø±"""
    query = update.callback_query
    await query.answer()
    
    if query.data == "new_link":
        await query.edit_message_text("ğŸ”„ *Ø£Ø±Ø³Ù„ Ø±Ø§Ø¨Ø· Ø§Ù„Ø­Ù„Ù‚Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©...*\n\nØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø±Ø§Ø¨Ø· Ù…Ù† Ù…ÙˆÙ‚Ø¹ Ø¹Ø±Ø¨ Ø³ÙŠØ¯ ÙˆÙŠØ¨Ø¯Ø£ Ø¨Ù€ https://", parse_mode='Markdown')

async def status_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """ÙØ­Øµ Ø­Ø§Ù„Ø© Ø§Ù„Ø¨ÙˆØª"""
    status_text = """
âœ… *Ø§Ù„Ø¨ÙˆØª ÙŠØ¹Ù…Ù„ Ø¨Ø´ÙƒÙ„ Ø·Ø¨ÙŠØ¹ÙŠ*

ğŸ¤– *Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¨ÙˆØª:*
â€¢ Ø§Ù„Ø­Ø§Ù„Ø©: ğŸŸ¢ Ù†Ø´Ø·
â€¢ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†: {}
â€¢ ÙŠØ¹Ù…Ù„ Ù…Ù†Ø°: {}

âš¡ *Ø¢Ø®Ø± ØªØ­Ø¯ÙŠØ«:* {}
    """.format(
        len(user_sessions),
        datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        datetime.now().strftime("%H:%M:%S")
    )
    
    await update.message.reply_text(status_text, parse_mode='Markdown')

async def error_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡"""
    logger.error(f"âŒ Ø®Ø·Ø£: {context.error}")
    
    try:
        if update and update.effective_message:
            await update.effective_message.reply_text("âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ØŒ Ø¬Ø§Ø±ÙŠ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©...")
    except:
        pass

# ----------------- Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ -----------------
def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    print("=" * 50)
    print("ğŸ¬ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø¨ÙˆØª Ø¹Ø±Ø¨ Ø³ÙŠØ¯ Telegram")
    print("=" * 50)
    
    try:
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
        application = Application.builder().token(TOKEN).build()
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø§Øª
        application.add_handler(CommandHandler("start", start_command))
        application.add_handler(CommandHandler("help", help_command))
        application.add_handler(CommandHandler("status", status_command))
        application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))
        application.add_handler(CallbackQueryHandler(handle_callback))
        
        # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
        application.add_error_handler(error_handler)
        
        # Ø¨Ø¯Ø¡ Ø§Ù„Ø¨ÙˆØª
        print("âœ… Ø§Ù„Ø¨ÙˆØª Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„!")
        print("ğŸ“± Ø§ÙØªØ­ Telegram ÙˆØ§Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¨ÙˆØª")
        print("âš¡ Ø£Ø±Ø³Ù„ /start Ù„Ù„Ø¨Ø¯Ø¡")
        
        application.run_polling(
            allowed_updates=Update.ALL_TYPES,
            drop_pending_updates=True,
            close_loop=False
        )
        
    except Exception as e:
        print(f"âŒ ÙØ´Ù„ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨ÙˆØª: {e}")
        logger.error(f"ÙØ´Ù„ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¨ÙˆØª: {e}\n{traceback.format_exc()}")

if __name__ == "__main__":
    main()
